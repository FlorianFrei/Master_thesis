---
title: "R_Master"
output: ''
date: "2024-08-13"
---
```{r}
library(tidyverse)
source("E:/Florian/Github/Master_thesis/Analysis/Data_wrangling/wrangling_helper.R")
```

```{r}
Data <- load_opto(datapath = "E:/Florian/Aligned_Data/Analysable_data/Data",metapath = "E:/Florian/Aligned_Data/Analysable_data/meta")
combcont <- Align_all(Data)
Trialless <- avg_trials(combcont, bin_size = 0.01)

Data %>% write_csv("C:/Users/deepl/Desktop/try_wrongh_neuro/wrangled_Data/Data.csv")
combcont %>% write_csv("C:/Users/deepl/Desktop/try_wrongh_neuro/wrangled_Data/combcont.csv")
Trialless %>% write_csv("C:/Users/deepl/Desktop/try_wrongh_neuro/wrangled_Data/Trialless.csv")


 list_of_data <- list.files(path = "E:/Florian/Aligned_Data/Analysable_data/Data",
                             pattern = "\\.csv$",
                             full.names = TRUE)
 

library(data.table)

# Function to process each file
process_csv_remove_first_column <- function(file_path) {
  # Read the CSV
  data <- fread(file_path)
  
  # Check if the first column exists
  if (ncol(data) > 0) {
    # Remove the first column
    data <- data[, -1, with = FALSE]
    # Save the modified data back to the same file
    fwrite(data, file_path)
    message(paste("Processed and saved:", file_path))
  } else {
    message(paste("No columns to process in:", file_path))
  }
}

# List of file paths (update this with your list of file paths)

# Apply the function to each file
lapply(list_of_data, process_csv_remove_first_column )

```


```{r}


Data %>% write_csv("E:/Florian/Aligned_Data/Neuropixles_analysable/wrangled_Data/Data.csv")
combcont %>% write_csv("E:/Florian/Aligned_Data/Neuropixles_analysable/wrangled_Data/combcont.csv")
Trialless %>% write_csv("E:/Florian/Aligned_Data/Neuropixles_analysable/wrangled_Data/Trialless.csv")

Data <- data.table::fread("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/R_outputs/final_combined_data.csv")

Data %>% filter(event_count > 0) %>% tidyr::separate_wider_delim(id, delim = '_', names = c('Mouse','Day')) %>%  select(Mouse,Day, cluster_id,time_bin,event_count,Trialtype,Behv,Trial)  %>% uncount(weights = event_count) %>% mutate( time_bin = time_bin  + runif(n(), min = 0.001, max = 0.002) ) %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/R_outputs/Zetadata.csv")

```

```{r}
Data <- data.table::fread("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/Data.csv")
Data %>% filter(event_count > 0) %>% tidyr::separate_wider_delim(id, delim = '_', names = c('Mouse','Day')) %>%  select(Mouse,Day, cluster_id,time_bin,event_count,Trialtype,Behv,Trial)  %>% uncount(weights = event_count) %>% mutate( time_bin = time_bin  + runif(n(), min = 0.001, max = 0.002) ) %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/Zetadata.csv")

contlick <-Align_behv(Data = Data %>% filter(Behv =='A2L'),State = 'HIT') %>% mutate(State = 'Lick')
contlick %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/contlick.csv") 


Data <- data.table::fread("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/Data.csv")
Data %>% distinct(Trialtype)
contlick_w2T <-Align_behv(Data = Data %>% filter(Behv =='W2T'),State = 'LeftReward') %>% mutate(State = 'Lick')
contlick_w2T  %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/contlick_W2T.csv") 
```


```{r}
#Data <- read.csv("E:/Florian/Aligned_Data/All_sessions/wrangled_Data/Data.csv")
final_data
contAir <-Align_behv(Data = final_data %>% filter(Behv =='A2L'),State = 'Airpuff') %>% mutate(State = 'Airpuff')
contAir %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_data/contAir.csv") 
rm(contAir)

contWT_Hits <- Align_behv(Data = final_data %>% filter(Behv =='W2T' & succ =='Hit'),State = 'HIT') %>% mutate(State = 'Hit')
contWT_Hits %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_data/contW2T.csv")
rm(contWT_Hits )

contPC <- Align_behv(Data = final_data %>% filter(Behv =='PC'),State = 'Airpuff2') %>% mutate(State = 'PC')
contPC %>%  write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_data/contPC.csv")
rm(contWT_)

rm(Data)
contAir <- read_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_data/contAir.csv")
contWT_Hits <- read_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_data/contW2T.csv")
contPC <- read_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_data")

 
combcont <- rbind(contAir,contWT_Hits,contPC)
combcont <- combcont %>% left_join(combcont %>% group_by(id,Behv) %>% distinct(Trial) %>% summarise(n_trials = n()))
combcont %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/combcont.csv")



Trialless <- avg_trials(combcont, bin_size = 0.01)
Trialless %>% write_csv("E:/Florian/Aligned_Data/Analysable_data/wrangled_Data/Trialless.csv")

combcont %>% distinct(Trialtype)



combcont %>% select(Trialtype,n_spikes,depth,group,fr,succ,n_trials)

combcont %>% group_by(Behv,rel_time, cluster_id2)
```
```{r}
Data2 <- Data %>% mutate(align = ifelse(Trialtype %in% c('AirBehind','AirTop_noOpto','AirTop_Opto','AirTop_Opto_late'),'airpuff','fuck')) 

filter2 <- Data2 %>% group_by(Trial2) %>%  filter(align=='airpuff') %>% slice_min(time_bin) %>% mutate(minT = time_bin) %>% select(minT,id,Trial) %>% distinct()
Data2 <- Data2  %>% left_join(filter2) %>% mutate(rel_time = round(time_bin - minT,2) )

combcont <- Data2 %>% left_join(Data2 %>% group_by(id,Behv) %>% distinct(Trial) %>% summarise(n_trials = n()))

 Trialless <- combcont %>% group_by(Behv,rel_time, cluster_id2)  %>% 
    mutate(rate = (sum(event_count)/n_trials), rate = rate/0.01) %>% ungroup() %>% 
    distinct(Behv,rel_time, cluster_id2,id,rate,n_spikes,depth,group,fr,n_trials,phase) %>% group_by(Behv,cluster_id2) %>% 
    mutate(Zscore = (rate - mean(rate))/sd(rate)) %>% ungroup()
  

```



`
```{r}
library(tidyverse)

load_data <- function(datapath, metapath, output_folder, chunk_size = 1) {
  # Ensure the output folder exists
  if (!dir.exists(output_folder)) {
    dir.create(output_folder)
  }

  # Helper function to process and save a chunk of CSV files
  process_and_save_chunk <- function(file_chunk, meta, output_folder, chunk_index) {
    # Load and process all files in the chunk
    chunk_data <- file_chunk %>%
      map_dfr(~ read_csv(.x, id = 'group')) %>%
      mutate(id = str_extract(group, "M\\d+_\\d+"))

    # Add meta data
    chunk_data <- chunk_data %>%
      select(cluster_id:trial, id) %>%
      left_join(meta %>% select(-id2), by = join_by(id, cluster_id)) %>%
      mutate(
        Trial = sprintf('%03d', trial)) %>%  unite('Trial2', c(id, Trial), remove = FALSE) %>% group_by(Trial2) %>% 
      mutate(
        Behv = case_when(
          any(Trialtype %in% c('Audio', 'W2T_Audio')) ~ "W2T",
          any(Trialtype %in% c('Airpuff', 'A2L_Audio')) ~ "A2L",
          any(Trialtype %in% c('PC_Audio', 'Airpuff2')) ~ "PC",
          .default = "WTF"
        ),
        succ = ifelse('HIT' %in% Trialtype, 'Hit', 'Miss')
      ) %>% ungroup() %>%
      unite('cluster_id2', c(cluster_id, id), remove = FALSE)

    # Save chunk to output folder
    chunk_name <- paste0(output_folder, "/chunk_", chunk_index, ".csv")
    write_csv(chunk_data, chunk_name)
    message(paste("Processed and saved chunk:", chunk_name))
    
    rm(chunk_data)
    gc() # Trig
  }

  # Load and prepare meta data
  list_of_meta <- list.files(path = metapath, pattern = "\\.tsv$", full.names = TRUE)
  meta <- map_dfr(list_of_meta, read_tsv, id = 'id2') %>%
    as_tibble() %>%
    select(id2, cluster_id, n_spikes, depth, group, fr) %>%
    mutate(id = str_extract(id2, "M\\d+_\\d+"))

  # Get list of CSV files
  list_of_data <- list.files(path = datapath, pattern = "\\.csv$", full.names = TRUE)

  # Process files in chunks
  chunk_index <- 1
  for (i in seq(1, length(list_of_data), by = chunk_size)) {
    file_chunk <- list_of_data[i:min(i + chunk_size - 1, length(list_of_data))]
    process_and_save_chunk(file_chunk, meta, output_folder, chunk_index)
    chunk_index <- chunk_index + 1
  }

  # Combine all processed chunks into a single data frame
  processed_files <- list.files(output_folder, pattern = "\\.csv$", full.names = TRUE)
  final_data <- map_dfr(processed_files, read_csv)

  message("All files processed and combined.")
  return(final_data)
}


datapath = "E:/Florian/Aligned_Data/Analysable_data/Data"
metapath = "E:/Florian/Aligned_Data/Analysable_data/meta"
output_folder <- "E:/Florian/Aligned_Data/Analysable_data/wrangled_data"

# Call the function
final_data <- load_data(datapath, metapath, output_folder, chunk_size = 2)

# Save the final combined data if needed
write_csv(final_data, "final_combined_data.csv")



```
