{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666af60-e00b-4e62-8063-1841fe18c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import probeinterface as pi\n",
    "from pathlib import Path\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "basefolder=\"C:/Users/Freitag/Desktop/timetest3_g0\"\n",
    "\n",
    "recording =  si.read_spikeglx(basefolder, stream_id='nidq', load_sync_channel=False)\n",
    "lfp = si.read_spikeglx(basefolder, stream_id='imec0.lf', load_sync_channel=False)\n",
    "event =  si.read_spikeglx(basefolder, stream_id='nidq', load_sync_channel=False)\n",
    "print(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3caad-5981-40cb-93e2-da6bbd9e03b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get sampling frequency\n",
    "channel_idx = 0# or 1, 2, etc., depending on which channel you want\n",
    "channel_id = event.get_channel_ids()[channel_idx]\n",
    "\n",
    "# Get trace from that single channel (e.g., first 5 seconds)\n",
    "sf = event.get_sampling_frequency()\n",
    "duration = 100  # seconds\n",
    "trace = event.get_traces(\n",
    "    start_frame=0,\n",
    "    end_frame=int(sf * duration),\n",
    "    channel_ids=[channel_id]\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(trace[:, 0])\n",
    "plt.title(f\"Analog signal from channel {channel_id}\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Signal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff0831-eda7-49c0-8dae-b8a6ba416a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "channel_ids = event.get_channel_ids()\n",
    "ttl_channel_idx = 0\n",
    "trace = event.get_traces(channel_ids=[channel_ids[ttl_channel_idx]])\n",
    "\n",
    "# Detect rising edges (assuming TTL goes from 0 to high value)\n",
    "threshold = 2500  # Adjust based on signal\n",
    "above_thresh = trace[:, 0] > threshold\n",
    "rising_edges = np.where(np.diff(above_thresh.astype(int)) == 1)[0]\n",
    "\n",
    "# Convert to times in seconds\n",
    "rising_times = rising_edges / sf\n",
    "print(\"TTL rising edge times:\", rising_times)\n",
    "print(len(rising_times))\n",
    "df = pd.DataFrame(rising_times)\n",
    "df.to_csv('C:/Users/Freitag/Desktop/soundtry.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093d713-b75d-4029-a737-e17c8e1d44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "channel_ids = event.get_channel_ids()\n",
    "ttl_channel_idx = 8\n",
    "trace = event.get_traces(channel_ids=[channel_ids[ttl_channel_idx]])\n",
    "\n",
    "# Detect rising edges (assuming TTL goes from 0 to high value)\n",
    "threshold = 0.5 # Adjust based on signal\n",
    "above_thresh = trace[:, 0] > threshold\n",
    "rising_edges = np.where(np.diff(above_thresh.astype(int)) == 1)[0]\n",
    "\n",
    "# Convert to times in seconds\n",
    "rising_times = rising_edges / sf\n",
    "print(\"TTL rising edge times:\", rising_times)\n",
    "print(len(rising_times))\n",
    "df = pd.DataFrame(rising_times)\n",
    "df.to_csv('C:/Users/Freitag/Desktop/digital1.csv')\n",
    "\n",
    "\n",
    "def extract_and_save_ttl_events(data, bits, save_path):\n",
    "    digital_signals = data.get_traces()\n",
    "    digital_word = digital_signals[:, 8]\n",
    "    print(digital_word)\n",
    "    sampling_rate = data.get_sampling_frequency()\n",
    "    for bit in bits:\n",
    "        # Extract TTL pulses for the current bit\n",
    "        ttl_timestamps = extract_ttl_from_bit(digital_word, bit, sampling_rate)\n",
    "        \n",
    "        ttl_df = pd.DataFrame(ttl_timestamps, columns=['timestamps'])\n",
    "        \n",
    "        filename = f'ttl_{bit}.csv'\n",
    "        \n",
    "        ttl_df.to_csv(f\"{save_path}/{filename}\", index=False)\n",
    "        print(f\"Extracted TTL event timestamps for bit {bit} saved to {filename}\")\n",
    "\n",
    "\n",
    "def extract_ttl_from_bit(digital_word, bit, sampling_rate):\n",
    "    # Extract the specific bit from the word (bit-shifting and masking)\n",
    "    ttl_signal = (digital_word >> bit) & 1  # Isolate bit\n",
    "    \n",
    "    # Define how many samples to plot (e.g., 100,000 samples)\n",
    "    num_samples_to_plot = 1000000\n",
    "    time_axis = np.arange(num_samples_to_plot) / sampling_rate  # Convert to seconds\n",
    "\n",
    "    # --- Add plotting ---\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.plot(time_axis, ttl_signal[:num_samples_to_plot])\n",
    "    plt.title(f'Isolated Bit {bit} State (0 or 1)')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Bit State')\n",
    "    plt.ylim(-0.1, 1.5)\n",
    "    plt.xlim(0, num_samples_to_plot / sampling_rate)\n",
    "    plt.show()\n",
    "    # --- End plotting ---\n",
    "    \n",
    "    # Detect rising edges (0 -> 1 transitions)\n",
    "    ttl_rising_edges = np.where(np.diff(ttl_signal) > 0)[0]\n",
    "\n",
    "    # Convert sample indices to timestamps (in seconds)\n",
    "    ttl_timestamps = ttl_rising_edges / sampling_rate\n",
    "    \n",
    "    return ttl_timestamps\n",
    "\n",
    "\n",
    "bits_to_extract = [0]  \n",
    "extract_and_save_ttl_events(event , bits_to_extract, 'C:/Users/Freitag/Desktop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368c676-ffec-4a2e-9a67-03d8dd832134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parameters ---\n",
    "# Adjust these based on your signal characteristics and sampling frequency\n",
    "\n",
    "# Preprocessing\n",
    "smoothing_window_ms = 1  # Smoothing window in milliseconds (adjust as needed, 0 to disable)\n",
    "\n",
    "# Detection\n",
    "threshold_factor = 1000 # How many times above noise level to set threshold (adjust based on SNR)\n",
    "debounce_interval_ms = 600 # Minimum time between detected events (ms) - should be < 500ms\n",
    "\n",
    "# Hierarchy Identification\n",
    "expected_sub_repeat_interval_s = 1 # Expected time between sub-repeats (seconds)\n",
    "sub_repeat_interval_tolerance_s = 0.090 # Tolerance for sub-repeat interval (seconds)\n",
    "# Assume gap between main repeats is significantly larger than sub-repeat gap + tolerance\n",
    "main_repeat_separation_threshold_s = 2.9 # e.g., > 0.8s\n",
    "\n",
    "# --- Assume 'recording' is your loaded SpikeInterface Recording object ---\n",
    "# Example loading (replace with your actual loading code)\n",
    "# recording = se.read_nidq('path/to/your/nidq/recording') \n",
    "# Or if already loaded:\n",
    "# recording = my_loaded_recording_object \n",
    "\n",
    "# --- Make sure you have a recording object for this example ---\n",
    "# Creating a dummy recording for demonstration if you don't have one loaded\n",
    "try:\n",
    "    recording # Check if recording exists\n",
    "except NameError:\n",
    "    print(\"Creating a dummy recording for demonstration purposes.\")\n",
    "    sampling_frequency = 30000 # Hz\n",
    "    traces = np.random.randn(sampling_frequency * 60, 1) * 0.1 # 60 seconds of noise\n",
    "    # Add some simulated events (replace with your actual data)\n",
    "    event_times_s = []\n",
    "    t_start = 5.0\n",
    "    for main_rep in range(3): # Simulate 3 main repeats instead of 25\n",
    "        main_start = t_start\n",
    "        for sub_rep in range(5):\n",
    "             start_idx = int((main_start + sub_rep * 0.6) * sampling_frequency) # Approx 0.5s silence + 0.1s signal\n",
    "             end_idx = start_idx + int(0.1 * sampling_frequency) # 100ms signal\n",
    "             if end_idx < len(traces):\n",
    "                 traces[start_idx:end_idx, 0] += np.sin(np.linspace(0, 10*np.pi, end_idx-start_idx)) * 1.0 # Add signal burst\n",
    "             event_times_s.append(main_start + sub_rep * 0.6)\n",
    "        t_start += 5 * 0.6 + 1.0 # Add longer gap between main repeats\n",
    "    recording = se.NumpyRecording([traces], sampling_frequency=sampling_frequency)\n",
    "    print(f\"Dummy event times: {event_times_s}\")\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# --- Select the relevant channel ---\n",
    "# If your NIDAQ signal is on a specific channel, select it.\n",
    "# Replace 'channel_id_of_interest' with the actual ID if known.\n",
    "# If it's single channel, channel_ids[0] should work.\n",
    "if len(recording.get_channel_ids()) > 1:\n",
    "    # Option 1: Choose a specific channel ID (replace 'your_channel_id')\n",
    "    # channel_id_to_use = 'your_channel_id' \n",
    "    # Option 2: Just use the first channel (common for Aux signals)\n",
    "    channel_id_to_use = recording.get_channel_ids()[0] \n",
    "    print(f\"Using channel ID: {channel_id_to_use}\")\n",
    "elif len(recording.get_channel_ids()) == 1:\n",
    "     channel_id_to_use = recording.get_channel_ids()[0]\n",
    "     print(f\"Using the only available channel ID: {channel_id_to_use}\")\n",
    "else:\n",
    "    raise ValueError(\"Recording has no channels!\")\n",
    "\n",
    "# Ensure we work with a single channel recording view\n",
    "recording_ch = recording.channel_slice([channel_id_to_use])\n",
    "\n",
    "# --- Get Signal Data ---\n",
    "fs = recording_ch.get_sampling_frequency()\n",
    "print(f\"Sampling frequency: {fs} Hz\")\n",
    "\n",
    "# Process in chunks if the recording is very long to avoid memory issues\n",
    "# For simplicity, loading all at once here. Adapt if needed.\n",
    "print(\"Loading traces...\")\n",
    "signal = recording_ch.get_traces(segment_index=0, return_scaled=True).flatten()\n",
    "times = recording_ch.get_times(segment_index=0)\n",
    "print(f\"Signal loaded. Duration: {times[-1]:.2f} s, Samples: {len(signal)}\")\n",
    "\n",
    "# --- 1. Preprocessing ---\n",
    "print(\"Preprocessing signal...\")\n",
    "# Rectify (take absolute value)\n",
    "processed_signal = np.abs(signal)\n",
    "\n",
    "# Optional: Smoothing (moving average)\n",
    "window_samples = int(smoothing_window_ms * fs / 1000)\n",
    "if window_samples > 1:\n",
    "    print(f\"Applying moving average smoothing with window: {window_samples} samples ({smoothing_window_ms} ms)\")\n",
    "    processed_signal = np.convolve(processed_signal, np.ones(window_samples)/window_samples, mode='same')\n",
    "else:\n",
    "    print(\"Smoothing disabled or window too small.\")\n",
    "\n",
    "# --- 2. Thresholding ---\n",
    "print(\"Calculating detection threshold...\")\n",
    "# Estimate noise level using Median Absolute Deviation (MAD) for robustness\n",
    "median_val = np.median(processed_signal)\n",
    "mad = np.median(np.abs(processed_signal - median_val))\n",
    "\n",
    "if mad == 0:\n",
    "    # Handle cases with very little noise or flat signal\n",
    "    noise_level_est = np.std(processed_signal) \n",
    "    print(f\"Warning: MAD is zero. Using standard deviation {noise_level_est:.4g} as noise estimate.\")\n",
    "    if noise_level_est == 0:\n",
    "         raise ValueError(\"Signal appears to be completely flat. Cannot set threshold.\")\n",
    "else:\n",
    "    # Convert MAD to equivalent standard deviation for Gaussian noise\n",
    "    noise_level_est = mad * 1.4826 \n",
    "    \n",
    "threshold = median_val + threshold_factor * noise_level_est\n",
    "print(f\"Estimated noise level (robust std dev): {noise_level_est:.4g}\")\n",
    "print(f\"Calculated threshold: {threshold:.4g}\")\n",
    "\n",
    "# Find samples where the signal crosses *above* the threshold\n",
    "above_threshold = processed_signal > threshold\n",
    "crossings_indices = np.where(np.diff(above_threshold.astype(int)) > 0)[0] + 1 # Add 1 to get the first index *above* threshold\n",
    "\n",
    "print(f\"Found {len(crossings_indices)} potential threshold crossings.\")\n",
    "\n",
    "# --- 3. Debouncing ---\n",
    "print(\"Debouncing events...\")\n",
    "debounce_samples = int(debounce_interval_ms * fs / 1000)\n",
    "if len(crossings_indices) > 0:\n",
    "    event_indices = [crossings_indices[0]]\n",
    "    for idx in crossings_indices[1:]:\n",
    "        if idx - event_indices[-1] >= debounce_samples:\n",
    "            event_indices.append(idx)\n",
    "    event_indices = np.array(event_indices)\n",
    "else:\n",
    "    event_indices = np.array([])\n",
    "\n",
    "event_times = event_indices / fs # Convert sample indices to time in seconds\n",
    "print(f\"Found {len(event_times)} events after debouncing.\")\n",
    "\n",
    "# --- 4. Hierarchical Grouping ---\n",
    "print(\"Identifying main and sub-repeat structure...\")\n",
    "sub_repeat_start_times = [] # List of arrays, each array contains times for one main repeat\n",
    "main_repeat_start_times = []\n",
    "\n",
    "if len(event_times) > 0:\n",
    "    # First event always starts a main repeat and its first sub-repeat\n",
    "    main_repeat_start_times.append(event_times[0])\n",
    "    current_sub_group = [event_times[0]]\n",
    "\n",
    "    if len(event_times) > 1:\n",
    "        inter_event_intervals = np.diff(event_times)\n",
    "\n",
    "        for i, interval in enumerate(inter_event_intervals):\n",
    "            current_event_time = event_times[i + 1]\n",
    "\n",
    "            # Check if interval suggests a new main repeat\n",
    "            if interval > main_repeat_separation_threshold_s:\n",
    "                # End of the previous main repeat's sub-group\n",
    "                sub_repeat_start_times.append(np.array(current_sub_group))\n",
    "                # Start of a new main repeat\n",
    "                main_repeat_start_times.append(current_event_time)\n",
    "                current_sub_group = [current_event_time] # Start new sub-group\n",
    "            # Check if interval matches expected sub-repeat separation\n",
    "            elif abs(interval - expected_sub_repeat_interval_s) <= sub_repeat_interval_tolerance_s:\n",
    "                current_sub_group.append(current_event_time)\n",
    "            else:\n",
    "                # Interval doesn't fit either pattern\n",
    "                print(f\"Warning: Unexpected interval {interval:.3f}s between event at {event_times[i]:.3f}s and {current_event_time:.3f}s. \"\n",
    "                      f\"This event might be noise or indicate timing issues. It won't be grouped.\")\n",
    "                # Decide how to handle - here we just don't add it to the current_sub_group\n",
    "\n",
    "    # Add the last collected sub-group\n",
    "    if current_sub_group:\n",
    "        sub_repeat_start_times.append(np.array(current_sub_group))\n",
    "\n",
    "# Convert main repeats list to numpy array for consistency\n",
    "main_repeat_start_times = np.array(main_repeat_start_times)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Detected {len(main_repeat_start_times)} main repeats.\")\n",
    "print(f\"Detected {sum(len(sub_group) for sub_group in sub_repeat_start_times)} total sub-repeats.\")\n",
    "\n",
    "print(\"\\nStart times of Main Repeats (seconds):\")\n",
    "print(main_repeat_start_times)\n",
    "\n",
    "print(\"\\nStart times of Sub-Repeats within each Main Repeat (seconds):\")\n",
    "for i, sub_group in enumerate(sub_repeat_start_times):\n",
    "    print(f\"  Main Repeat {i+1} (starts at {main_repeat_start_times[i]:.3f}s):\")\n",
    "    print(f\"    {sub_group}\")\n",
    "\n",
    "# --- Optional: Verification Plot ---\n",
    "print(\"\\nGenerating verification plot...\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(times- min(times),signal, label='Processed Signal', alpha=0.7, color='gray')\n",
    "plt.axhline(threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.2f})')\n",
    "\n",
    "# Plot detected events (all after debouncing)\n",
    "plt.plot(event_times, processed_signal[event_indices], 'o', color='orange', markersize=8, label='Detected Events (All)')\n",
    "\n",
    "# Mark main repeat starts differently\n",
    "if len(main_repeat_start_times) > 0:\n",
    "     main_repeat_indices = np.searchsorted(times, main_repeat_start_times)\n",
    "     # Clip indices to be within bounds\n",
    "     main_repeat_indices = np.clip(main_repeat_indices, 0, len(processed_signal) - 1)\n",
    "     plt.plot(main_repeat_start_times, processed_signal[main_repeat_indices], '^', color='green', markersize=10, label='Main Repeat Starts')\n",
    "\n",
    "\n",
    "plt.title('Detected Audio Events on Processed NIDAQ Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Processed Signal Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='x', linestyle=':', alpha=0.7)\n",
    "# Optional: Zoom into a specific time range if needed\n",
    "plt.xlim(9.13, 9.145)\n",
    "#plt.ylim(-3000,3000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd42b54-6793-46a9-bc8c-222b26f6c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Add this import at the top of your script if not already there\n",
    "import os # To handle file paths\n",
    "\n",
    "# --- Add this section after the results printing and plotting ---\n",
    "\n",
    "print(\"\\n--- Exporting Results to CSV ---\")\n",
    "\n",
    "# Define output filenames\n",
    "output_dir = \"C:/Users/Freitag/Desktop\" # Save in the current directory, change if needed\n",
    "combined_csv_filename = os.path.join(output_dir, \"audio_event_times_combined.csv\")\n",
    "# Optional: Define names if saving as two separate files\n",
    "# main_repeats_csv_filename = os.path.join(output_dir, \"main_repeat_times.csv\")\n",
    "# sub_repeats_csv_filename = os.path.join(output_dir, \"sub_repeat_times_detailed.csv\")\n",
    "\n",
    "\n",
    "# --- Option 1: Save as a Single Combined CSV (Recommended) ---\n",
    "# Structure: Each row is a sub-repeat, with info about its parent main repeat.\n",
    "event_data_list = []\n",
    "if len(main_repeat_start_times) > 0 and len(sub_repeat_start_times) == len(main_repeat_start_times):\n",
    "    for i, main_start_time in enumerate(main_repeat_start_times):\n",
    "        # Ensure we have sub-repeat data for this main repeat\n",
    "        if i < len(sub_repeat_start_times):\n",
    "             current_sub_group = sub_repeat_start_times[i]\n",
    "             for j, sub_start_time in enumerate(current_sub_group):\n",
    "                 event_data_list.append({\n",
    "                     'main_repeat_index': i + 1, # 1-based index for main repeat\n",
    "                     'main_repeat_start_time_s': main_start_time,\n",
    "                     'sub_repeat_index_within_main': j + 1, # 1-based index for sub-repeat within main\n",
    "                     'sub_repeat_start_time_s': sub_start_time\n",
    "                 })\n",
    "        else:\n",
    "             print(f\"Warning: Missing sub-repeat data for main repeat index {i} (start time {main_start_time}). Skipping.\")\n",
    "             # Optionally add a row indicating the main repeat was found but sub-repeats were missing/problematic\n",
    "             # event_data_list.append({\n",
    "             #     'main_repeat_index': i + 1,\n",
    "             #     'main_repeat_start_time_s': main_start_time,\n",
    "             #     'sub_repeat_index_within_main': None, # Or 0, or NaN\n",
    "             #     'sub_repeat_start_time_s': None # Or NaN\n",
    "             # })\n",
    "\n",
    "\n",
    "    if event_data_list:\n",
    "        # Create DataFrame\n",
    "        combined_df = pd.DataFrame(event_data_list)\n",
    "        \n",
    "        # Save to CSV\n",
    "        try:\n",
    "            combined_df.to_csv(combined_csv_filename, index=False, float_format='%.6f')\n",
    "            print(f\"Successfully saved combined event data to: {combined_csv_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving combined CSV file: {e}\")\n",
    "    else:\n",
    "        print(\"No event data structures generated to save.\")\n",
    "        \n",
    "elif len(main_repeat_start_times) > 0:\n",
    "     print(\"Warning: Mismatch between number of main repeats and sub-repeat groups. Saving main repeats only.\")\n",
    "     # Fallback to saving just main repeats if sub-repeat structure is inconsistent\n",
    "     main_df = pd.DataFrame({'main_repeat_start_time_s': main_repeat_start_times})\n",
    "     try:\n",
    "        main_df.to_csv(os.path.join(output_dir, \"main_repeat_times_only.csv\"), index=False, float_format='%.6f')\n",
    "        print(f\"Successfully saved main repeat times only to: {os.path.join(output_dir, 'main_repeat_times_only.csv')}\")\n",
    "     except Exception as e:\n",
    "        print(f\"Error saving main repeats CSV file: {e}\")\n",
    "else:\n",
    "    print(\"No main repeats detected, skipping CSV export.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
