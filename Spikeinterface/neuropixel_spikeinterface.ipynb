{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdd5ee-49b3-476b-9b1e-e01c29890e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import probeinterface as pi\n",
    "from pathlib import Path\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "global_job_kwargs = dict(n_jobs=4, chunk_duration=\"1s\",progress_bar=True)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "\n",
    "\n",
    "basefolder=\"C:/Users/Freitag/Desktop/timetest_g0\"\n",
    "\n",
    "metapath = basefolder + str('/Meta')\n",
    "if not os.path.isdir(metapath):\n",
    "   os.makedirs(metapath)\n",
    "\n",
    "\n",
    "recording =  si.read_spikeglx(basefolder, stream_id='imec0.ap', load_sync_channel=False)\n",
    "lfp = si.read_spikeglx(basefolder, stream_id='imec0.lf', load_sync_channel=False)\n",
    "event =  si.read_spikeglx(basefolder, stream_id='nidq', load_sync_channel=False)\n",
    "print(recording)\n",
    "\n",
    "#recording = si.ChannelSliceRecording(recording, channel_ids=recording.get_channel_ids()[180:330])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e9e3-6c78-4798-ac9e-595a3a941744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bad_channel_ids, channel_labels = si.detect_bad_channels(lfp,method = 'coherence+psd',outside_channels_location = 'both')\n",
    "names = lfp.channel_ids\n",
    "depth = lfp.get_channel_locations()[:,1]\n",
    "\n",
    "\n",
    "ar = pd.DataFrame({'name':names, 'depth':depth, 'labels':channel_labels})\n",
    "ar.to_csv(metapath + str('/lfp_labels.csv'))\n",
    "filtered_ar = ar[ar['labels'] == 'out']\n",
    "\n",
    "print(filtered_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb0f77-af0d-4b43-abea-498a618c36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "si.plot_probe_map(recording4, ax=ax, with_channel_ids=True)\n",
    "ax.set_ylim(-200,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c2497-26c0-4677-bfce-5e38ba874eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1 = si.highpass_filter(recording, freq_min=400.)\n",
    "rec1 = si.phase_shift(rec1)\n",
    "bad_channel_ids, channel_labels = si.detect_bad_channels(rec1,method = 'coherence+psd')\n",
    "print(bad_channel_ids)\n",
    "rec1 = si.interpolate_bad_channels(recording=rec1, bad_channel_ids=bad_channel_ids)\n",
    "\n",
    "rec1 = si.common_reference(rec1, operator=\"median\", reference=\"global\")\n",
    "print(rec1)\n",
    "\n",
    "\n",
    "%matplotlib widget\n",
    "si.plot_traces({'raw':recording,'filtered':rec1}, backend='ipywidgets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b355e63-3f69-4455-bfab-dcdbb894e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_and_save_ttl_events(data, bits, save_path):\n",
    "    digital_signals = data.get_traces()\n",
    "    digital_word = digital_signals[:, 0]\n",
    "    sampling_rate = data.get_sampling_frequency()\n",
    "    for bit in bits:\n",
    "        # Extract TTL pulses for the current bit\n",
    "        ttl_timestamps = extract_ttl_from_bit(digital_word, bit, sampling_rate)\n",
    "        \n",
    "        ttl_df = pd.DataFrame(ttl_timestamps, columns=['timestamps'])\n",
    "        \n",
    "        filename = f'ttl_{bit}.csv'\n",
    "        \n",
    "        ttl_df.to_csv(f\"{save_path}/{filename}\", index=False)\n",
    "        print(f\"Extracted TTL event timestamps for bit {bit} saved to {filename}\")\n",
    "\n",
    "\n",
    "def extract_ttl_from_bit(digital_word, bit, sampling_rate):\n",
    "    # Extract the specific bit from the word (bit-shifting and masking)\n",
    "    ttl_signal = (digital_word >> bit) & 1  # Right shift and mask to isolate the specific bit\n",
    "    \n",
    "    # Detect rising edges (0 -> 1 transitions)\n",
    "    ttl_rising_edges = np.where(np.diff(ttl_signal) > 0)[0]\n",
    "    \n",
    "    # Convert sample indices to timestamps (in seconds)\n",
    "    ttl_timestamps = ttl_rising_edges / sampling_rate\n",
    "    \n",
    "    return ttl_timestamps\n",
    "\n",
    "\n",
    "bits_to_extract = [0, 1, 2,8]  \n",
    "extract_and_save_ttl_events(event , bits_to_extract, metapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0043c1-1de6-42c4-a1b1-4deb4ad76b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.sorters import installed_sorters\n",
    "installed_sorters()\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "Sorting_KS4 = si.run_sorter(sorter_name=\"kilosort4\", recording=rec1, folder=basefolder + str('/sorted'),remove_existing_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016961d-0522-49f7-a2aa-0bf95034e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting_KS4 = si.read_kilosort(folder_path=basefolder + str('/sorted/sorter_output'))\n",
    "analyzer = si.create_sorting_analyzer(Sorting_KS4, rec1, sparse=True, format=\"memory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e49ba9-f361-4caf-8cc7-a55f4f837daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.compute(['random_spikes', 'waveforms', 'templates', 'noise_levels','unit_locations','correlograms'],**global_job_kwargs)\n",
    "analyzer.compute('spike_amplitudes')\n",
    "analyzer.compute('principal_components', n_components = 5, mode=\"by_channel_local\",**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9df4a1-5841-488d-961f-4792f50375af",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names=['firing_rate', 'presence_ratio', 'snr','isi_violation', 'amplitude_cutoff']\n",
    "metrics = si.compute_quality_metrics(analyzer, metric_names=metric_names)\n",
    "\n",
    "\n",
    "amplitude_cutoff_thresh = 0.1\n",
    "isi_violations_ratio_thresh = 0.5\n",
    "presence_ratio_thresh = 0.9\n",
    "\n",
    "\n",
    "our_query = f\"(amplitude_cutoff < {amplitude_cutoff_thresh}) & (isi_violations_ratio < {isi_violations_ratio_thresh}) & (presence_ratio > {presence_ratio_thresh})\"\n",
    "\n",
    "keep_units = metrics.query(our_query)\n",
    "keep_unit_ids = keep_units.index.values\n",
    "analyzer_clean = analyzer.select_units(keep_unit_ids, folder=basefolder +str('/analyzer_clean'), format='binary_folder')\n",
    "print(analyzer)\n",
    "print(analyzer_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc149b7e-54e0-4181-b3e3-c0a767214c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_sorting_summary(sorting_analyzer=sorting_analyzer, curation=True, backend='spikeinterface_gui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4405f-72e9-432c-8f88-29136f61f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_to_phy(analyzer_clean, output_folder=basefolder + str('/sorted/phy'),**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f041d-f945-4b91-bf59-cc1a9456eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!phy template-gui E:\\Florian_paper\\Florian\\Data\\Opto_2\\O3_1\\sorted\\phy\\params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f026e-5889-48ac-8a70-46a5bab035a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spikeinterface.full as si  # Assuming `si` is the module for loading the analyzer\n",
    "\n",
    "import pandas as pd  # Ensure Pandas is imported\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path  # For modern path handling\n",
    "\n",
    "def find_key_in_dict(d, key_to_find):\n",
    "    for key, value in d.items():\n",
    "        if key == key_to_find:\n",
    "            # If the value is a Pandas Series, convert it to a list\n",
    "            if isinstance(value, pd.Series):\n",
    "                return value.tolist()\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            result = find_key_in_dict(value, key_to_find)\n",
    "            if result:\n",
    "                return result\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    result = find_key_in_dict(item, key_to_find)\n",
    "                    if result:\n",
    "                        return result\n",
    "    return None\n",
    "\n",
    "# Base directory where subfolders are located\n",
    "base_dir = Path(\"D:/Florian_paper/Florian/Data/Opto_2\")\n",
    "\n",
    "\n",
    "# Initialize a list to store all peak-to-valley values across subfolders\n",
    "all_peak_to_valley_ms = []\n",
    "original_indices = []  # To store the original indices for each value\n",
    "\n",
    "# Iterate through all subfolders\n",
    "for subfolder in base_dir.iterdir():  # Using pathlib to iterate\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            # Construct the full path and normalize it\n",
    "            analyzer_path = subfolder / \"analyzer_clean\"\n",
    "            analyzer_path = os.path.normpath(analyzer_path)  # Normalize the path\n",
    "            print(analyzer_path)\n",
    "            # Load the analyzer for the current subfolder\n",
    "            analyzer = si.load_sorting_analyzer(str(analyzer_path))  # Convert to string if needed\n",
    "            tm = analyzer.compute('template_metrics')\n",
    "            dat = tm.data\n",
    "\n",
    "            # Extract peak-to-valley data\n",
    "            peak_to_valley_data = find_key_in_dict(dat['metrics'], 'peak_to_valley')\n",
    "            \n",
    "            # Check if peak_to_valley_data is found\n",
    "            if peak_to_valley_data:\n",
    "                # If it's a Pandas Series, convert it to a list\n",
    "                if isinstance(peak_to_valley_data, pd.Series):\n",
    "                    peak_to_valley_data = peak_to_valley_data.tolist()\n",
    "                \n",
    "                # Convert to milliseconds and append to the cumulative list\n",
    "                peak_to_valley_ms = [value * 1000 for value in peak_to_valley_data]\n",
    "                \n",
    "                # Append the values and their original indices\n",
    "                all_peak_to_valley_ms.extend(peak_to_valley_ms)\n",
    "                original_indices.extend([f\"{subfolder.name}_row_{i}\" for i in range(len(peak_to_valley_ms))])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {subfolder}: {e}\")\n",
    "\n",
    "# Filter values below 0.425ms (450ms) from the list\n",
    "filtered_peak_to_valley_ms = [\n",
    "    (index, value) for index, value in zip(original_indices, all_peak_to_valley_ms) if value < 0.425\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the filtered data, including the original indices\n",
    "df_filtered = pd.DataFrame(filtered_peak_to_valley_ms, columns=[\"Original_Index\", \"Peak_to_Valley (ms)\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_filtered)\n",
    "\n",
    "df_filtered.to_csv(\"C:/Users/Freitag/Desktop/Opto2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18562c72-c526-4202-bc21-d165ffb6b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spikeinterface.full as si  # Assuming `si` is the module for loading the analyzer\n",
    "\n",
    "import pandas as pd  # Ensure Pandas is imported\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path  # For modern path handling\n",
    "\n",
    "def find_key_in_dict(d, key_to_find):\n",
    "    for key, value in d.items():\n",
    "        if key == key_to_find:\n",
    "            # If the value is a Pandas Series, convert it to a list\n",
    "            if isinstance(value, pd.Series):\n",
    "                return value.tolist()\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            result = find_key_in_dict(value, key_to_find)\n",
    "            if result:\n",
    "                return result\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    result = find_key_in_dict(item, key_to_find)\n",
    "                    if result:\n",
    "                        return result\n",
    "    return None\n",
    "base_dir = Path(\"E:/Florian/Data/batch1\")\n",
    "all_peak_to_valley_ms = []\n",
    "original_indices = []  \n",
    "\n",
    "for subfolder in base_dir.iterdir():  # Using pathlib to iterate\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            # Construct the path for sorting output (fixed)\n",
    "            sorting_output_path = subfolder / \"sorted/sorter_output\"\n",
    "            sorting_output_path = os.path.normpath(sorting_output_path)  # Normalize the path\n",
    "\n",
    "            # Search for a subfolder containing 'Record Node' and a number\n",
    "            record_node_folders = [f for f in subfolder.iterdir() if \"Record Node\" in f.name]\n",
    "            if not record_node_folders:\n",
    "                print(f\"No 'Record Node' found in {subfolder}. Skipping...\")\n",
    "                continue\n",
    "            record_node_folder = record_node_folders[0]\n",
    "            record_node_path = os.path.normpath(record_node_folder)  # Normalize the path\n",
    "            \n",
    "            # Print paths for debugging\n",
    "            print(f\"Sorting Output Path: {sorting_output_path}\")\n",
    "            print(f\"Record Node Path: {record_node_path}\")\n",
    "\n",
    "            # Load Sorting (Kilosort) data\n",
    "            Sorting_KS4 = si.read_kilosort(str(sorting_output_path))  # Ensure path is passed as string\n",
    "\n",
    "            # Load OpenEphys data\n",
    "            rec1 = si.read_openephys(str(record_node_path))\n",
    "\n",
    "            # Download probe\n",
    "            probe = pi.get_probe(manufacturer='cambridgeneurotech', probe_name='ASSY-77-H3')\n",
    "            \n",
    "            # Add wiring to device\n",
    "            probe.wiring_to_device('ASSY-77>Adpt.A64-Om32_2x-sm-NN>RHD2164')\n",
    "\n",
    "            # Set the probe for the current recording\n",
    "            rec1.set_probe(probe, group_mode=\"by_shank\", in_place=True)\n",
    "\n",
    "            # Create the sorting analyzer with the loaded data\n",
    "            analyzer = si.create_sorting_analyzer(Sorting_KS4, rec1, sparse=True, format=\"memory\")\n",
    "\n",
    "            # Compute the necessary metrics\n",
    "            global_job_kwargs = {}  # You can define the global arguments here if needed\n",
    "            analyzer.compute(['random_spikes', 'waveforms', 'templates'], **global_job_kwargs)\n",
    "\n",
    "            # Compute 'template_metrics'\n",
    "            tm = analyzer.compute('template_metrics')\n",
    "            dat = tm.data\n",
    "\n",
    "            peak_to_valley_data = find_key_in_dict(dat['metrics'], 'peak_to_valley')\n",
    "            \n",
    "            # Check if peak_to_valley_data is found\n",
    "            if peak_to_valley_data:\n",
    "                # If it's a Pandas Series, convert it to a list\n",
    "                if isinstance(peak_to_valley_data, pd.Series):\n",
    "                    peak_to_valley_data = peak_to_valley_data.tolist()\n",
    "                \n",
    "                # Convert to milliseconds and append to the cumulative list\n",
    "                peak_to_valley_ms = [value * 1000 for value in peak_to_valley_data]\n",
    "                \n",
    "                # Append the values and their original indices\n",
    "                all_peak_to_valley_ms.extend(peak_to_valley_ms)\n",
    "                original_indices.extend([f\"{subfolder.name}_row_{i}\" for i in range(len(peak_to_valley_ms))])\n",
    "        \n",
    "        except Exception as e:\n",
    "                print(f\"Error processing {subfolder}: {e}\")\n",
    "\n",
    "# Filter values below 0.425ms (450ms) from the list\n",
    "filtered_peak_to_valley_ms = [\n",
    "    (index, value) for index, value in zip(original_indices, all_peak_to_valley_ms) if value < 0.425\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the filtered data, including the original indices\n",
    "df_filtered = pd.DataFrame(filtered_peak_to_valley_ms, columns=[\"Original_Index\", \"Peak_to_Valley (ms)\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_filtered)\n",
    "\n",
    "df_filtered.to_csv(\"C:/Users/deepl/Desktop/Batch1.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sorting_KS4 = si.read_kilosort('path')  # Ensure path is passed as string\n",
    "recording =  si.read_spikeglx(basefolder, stream_id='imec1.ap', load_sync_channel=False)\n",
    "analyzer = si.create_sorting_analyzer(Sorting_KS4, rec1, sparse=True, format=\"memory\")\n",
    "analyzer.compute(['random_spikes', 'waveforms', 'templates'], **global_job_kwargs)\n",
    "\n",
    "            # Compute 'template_metrics'\n",
    "tm = analyzer.compute('template_metrics')\n",
    "dat = tm.data\n",
    "peak_to_valley_data = find_key_in_dict(dat['metrics'], 'peak_to_valley')\n",
    "            \n",
    "            # Check if peak_to_valley_data is found\n",
    "if peak_to_valley_data:\n",
    "                # If it's a Pandas Series, convert it to a list\n",
    "    if isinstance(peak_to_valley_data, pd.Series):\n",
    "        peak_to_valley_data = peak_to_valley_data.tolist()\n",
    "                \n",
    "                # Convert to milliseconds \n",
    "        peak_to_valley_ms = [value * 1000 for value in peak_to_valley_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7afbe0-05ab-4bfb-ba95-f66a897697a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spikeinterface.full as si  # Assuming `si` is the module for loading the analyzer\n",
    "import numpy as np\n",
    "import pandas as pd  # Ensure Pandas is imported\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path  # For modern path handling\n",
    "\n",
    "analyzer = si.load_sorting_analyzer(\"D:/Florian_paper/Florian/Data/batch3/M9_1/analyzer_clean\")  # Convert to string if needed\n",
    "#tm = analyzer.compute('template_metrics')\n",
    "\n",
    "unit_ids = analyzer.unit_ids\n",
    "\n",
    "si.plot_unit_templates(analyzer, unit_ids=[11, 15,94,\n",
    "                                          222,245,201], ncols=3, figsize=(32, 16))\n",
    "\n",
    "\n",
    "\n",
    "si.plot_unit_templates(analyzer, unit_ids=[94,222,245], ncols=3, figsize=(32, 16))\n",
    "plt.savefig('C:/Users/Freitag/Desktop/foo.tiff',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717be097-457e-4322-aa1e-d0eb62912128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
