{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdd5ee-49b3-476b-9b1e-e01c29890e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import probeinterface as pi\n",
    "from pathlib import Path\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "global_job_kwargs = dict(n_jobs=4, chunk_duration=\"1s\",progress_bar=True)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "\n",
    "\n",
    "basefolder=\"F:/copydaya/M7_1_copy\"\n",
    "\n",
    "metapath = basefolder + str('/Meta')\n",
    "if not os.path.isdir(metapath):\n",
    "   os.makedirs(metapath)\n",
    "\n",
    "\n",
    "recording =  si.read_spikeglx(basefolder, stream_id='imec1.ap', load_sync_channel=False)\n",
    "lfp = si.read_spikeglx(basefolder, stream_id='imec1.lf', load_sync_channel=False)\n",
    "event =  si.read_spikeglx(basefolder, stream_id='nidq', load_sync_channel=False)\n",
    "print(recording)\n",
    "\n",
    "#recording = si.ChannelSliceRecording(recording, channel_ids=recording.get_channel_ids()[180:330])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e9e3-6c78-4798-ac9e-595a3a941744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bad_channel_ids, channel_labels = si.detect_bad_channels(lfp,method = 'coherence+psd',outside_channels_location = 'both')\n",
    "names = lfp.channel_ids\n",
    "depth = lfp.get_channel_locations()[:,1]\n",
    "\n",
    "\n",
    "ar = pd.DataFrame({'name':names, 'depth':depth, 'labels':channel_labels})\n",
    "ar.to_csv(metapath + str('/lfp_labels.csv'))\n",
    "filtered_ar = ar[ar['labels'] == 'out']\n",
    "\n",
    "print(filtered_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb0f77-af0d-4b43-abea-498a618c36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "si.plot_probe_map(recording4, ax=ax, with_channel_ids=True)\n",
    "ax.set_ylim(-200,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c2497-26c0-4677-bfce-5e38ba874eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1 = si.highpass_filter(recording, freq_min=400.)\n",
    "rec1 = si.phase_shift(rec1)\n",
    "bad_channel_ids, channel_labels = si.detect_bad_channels(rec1,method = 'coherence+psd')\n",
    "print(bad_channel_ids)\n",
    "rec1 = si.interpolate_bad_channels(recording=rec1, bad_channel_ids=bad_channel_ids)\n",
    "\n",
    "rec1 = si.common_reference(rec1, operator=\"median\", reference=\"global\")\n",
    "print(rec1)\n",
    "\n",
    "\n",
    "%matplotlib widget\n",
    "si.plot_traces({'raw':recording,'filtered':rec1}, backend='ipywidgets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b355e63-3f69-4455-bfab-dcdbb894e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_and_save_ttl_events(data, bits, save_path):\n",
    "    digital_signals = data.get_traces()\n",
    "    digital_word = digital_signals[:, 0]\n",
    "    sampling_rate = data.get_sampling_frequency()\n",
    "    for bit in bits:\n",
    "        # Extract TTL pulses for the current bit\n",
    "        ttl_timestamps = extract_ttl_from_bit(digital_word, bit, sampling_rate)\n",
    "        \n",
    "        ttl_df = pd.DataFrame(ttl_timestamps, columns=['timestamps'])\n",
    "        \n",
    "        filename = f'ttl_{bit}.csv'\n",
    "        \n",
    "        ttl_df.to_csv(f\"{save_path}/{filename}\", index=False)\n",
    "        print(f\"Extracted TTL event timestamps for bit {bit} saved to {filename}\")\n",
    "\n",
    "\n",
    "def extract_ttl_from_bit(digital_word, bit, sampling_rate):\n",
    "    # Extract the specific bit from the word (bit-shifting and masking)\n",
    "    ttl_signal = (digital_word >> bit) & 1  # Right shift and mask to isolate the specific bit\n",
    "    \n",
    "    # Detect rising edges (0 -> 1 transitions)\n",
    "    ttl_rising_edges = np.where(np.diff(ttl_signal) > 0)[0]\n",
    "    \n",
    "    # Convert sample indices to timestamps (in seconds)\n",
    "    ttl_timestamps = ttl_rising_edges / sampling_rate\n",
    "    \n",
    "    return ttl_timestamps\n",
    "\n",
    "\n",
    "bits_to_extract = [0, 1, 2]  \n",
    "extract_and_save_ttl_events(event , bits_to_extract, metapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0043c1-1de6-42c4-a1b1-4deb4ad76b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.sorters import installed_sorters\n",
    "installed_sorters()\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "Sorting_KS4 = si.run_sorter(sorter_name=\"kilosort4\", recording=rec1, folder=basefolder + str('/sorted'),remove_existing_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016961d-0522-49f7-a2aa-0bf95034e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting_KS4 = si.read_kilosort(folder_path=basefolder + str('/sorted/sorter_output'))\n",
    "analyzer = si.create_sorting_analyzer(Sorting_KS4, rec1, sparse=True, format=\"memory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e49ba9-f361-4caf-8cc7-a55f4f837daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.compute(['random_spikes', 'waveforms', 'templates', 'noise_levels','unit_locations','correlograms'],**global_job_kwargs)\n",
    "analyzer.compute('spike_amplitudes')\n",
    "analyzer.compute('principal_components', n_components = 5, mode=\"by_channel_local\",**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9df4a1-5841-488d-961f-4792f50375af",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names=['firing_rate', 'presence_ratio', 'snr','isi_violation', 'amplitude_cutoff']\n",
    "metrics = si.compute_quality_metrics(analyzer, metric_names=metric_names)\n",
    "\n",
    "\n",
    "amplitude_cutoff_thresh = 0.1\n",
    "isi_violations_ratio_thresh = 0.5\n",
    "presence_ratio_thresh = 0.9\n",
    "\n",
    "\n",
    "our_query = f\"(amplitude_cutoff < {amplitude_cutoff_thresh}) & (isi_violations_ratio < {isi_violations_ratio_thresh}) & (presence_ratio > {presence_ratio_thresh})\"\n",
    "\n",
    "keep_units = metrics.query(our_query)\n",
    "keep_unit_ids = keep_units.index.values\n",
    "analyzer_clean = analyzer.select_units(keep_unit_ids, folder=basefolder +str('/analyzer_clean'), format='binary_folder')\n",
    "print(analyzer)\n",
    "print(analyzer_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc149b7e-54e0-4181-b3e3-c0a767214c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_sorting_summary(sorting_analyzer=sorting_analyzer, curation=True, backend='spikeinterface_gui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e4405f-72e9-432c-8f88-29136f61f9a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'si' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msi\u001b[49m\u001b[38;5;241m.\u001b[39mexport_to_phy(analyzer_clean, output_folder\u001b[38;5;241m=\u001b[39mbasefolder \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/sorted/phy\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mglobal_job_kwargs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'si' is not defined"
     ]
    }
   ],
   "source": [
    "si.export_to_phy(analyzer_clean, output_folder=basefolder + str('/sorted/phy'),**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19f041d-f945-4b91-bf59-cc1a9456eb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m16:21:18.498 [W] model:667            Skipping spike waveforms that do not exist, they will be extracted on the fly from the raw data as needed.\u001b[0m\n",
      "\u001b[0m16:24:15.780 [I] supervisor:711       Change metadata_group for clusters 0 to good.\u001b[0m\n",
      "\u001b[0m16:24:33.124 [I] supervisor:711       Change metadata_group for clusters 1 to good.\u001b[0m\n",
      "\u001b[0m16:24:41.683 [I] supervisor:711       Change metadata_group for clusters 2 to good.\u001b[0m\n",
      "\u001b[0m16:24:50.557 [I] supervisor:711       Change metadata_group for clusters 3 to mua.\u001b[0m\n",
      "\u001b[0m16:24:56.611 [I] supervisor:711       Change metadata_group for clusters 4 to good.\u001b[0m\n",
      "\u001b[0m16:25:03.452 [I] supervisor:711       Change metadata_group for clusters 5 to mua.\u001b[0m\n",
      "\u001b[0m16:25:09.843 [I] supervisor:711       Change metadata_group for clusters 6 to good.\u001b[0m\n",
      "\u001b[0m16:25:15.395 [I] supervisor:711       Change metadata_group for clusters 7 to good.\u001b[0m\n",
      "\u001b[0m16:25:19.988 [I] supervisor:711       Change metadata_group for clusters 8 to good.\u001b[0m\n",
      "\u001b[0m16:25:22.836 [I] supervisor:711       Change metadata_group for clusters 9 to good.\u001b[0m\n",
      "\u001b[0m16:25:26.196 [I] supervisor:711       Change metadata_group for clusters 10 to mua.\u001b[0m\n",
      "\u001b[0m16:25:32.627 [I] supervisor:711       Change metadata_group for clusters 11 to mua.\u001b[0m\n",
      "\u001b[0m16:25:37.243 [I] supervisor:711       Change metadata_group for clusters 12 to good.\u001b[0m\n",
      "\u001b[0m16:25:41.195 [I] supervisor:711       Change metadata_group for clusters 13 to good.\u001b[0m\n",
      "\u001b[0m16:25:45.339 [I] supervisor:711       Change metadata_group for clusters 14 to mua.\u001b[0m\n",
      "\u001b[0m16:25:48.780 [I] supervisor:711       Change metadata_group for clusters 15 to good.\u001b[0m\n",
      "\u001b[0m16:25:53.043 [I] supervisor:711       Change metadata_group for clusters 16 to mua.\u001b[0m\n",
      "\u001b[0m16:25:58.123 [I] supervisor:711       Change metadata_group for clusters 17 to noise.\u001b[0m\n",
      "\u001b[0m16:26:03.020 [I] supervisor:711       Change metadata_group for clusters 18 to mua.\u001b[0m\n",
      "\u001b[0m16:26:09.244 [I] supervisor:711       Change metadata_group for clusters 19 to noise.\u001b[0m\n",
      "\u001b[0m16:26:13.764 [I] supervisor:711       Change metadata_group for clusters 20 to good.\u001b[0m\n",
      "\u001b[0m16:26:17.299 [I] supervisor:711       Change metadata_group for clusters 21 to mua.\u001b[0m\n",
      "\u001b[0m16:26:21.164 [I] supervisor:711       Change metadata_group for clusters 22 to mua.\u001b[0m\n",
      "\u001b[0m16:26:24.244 [I] supervisor:711       Change metadata_group for clusters 23 to mua.\u001b[0m\n",
      "\u001b[0m16:26:27.773 [I] supervisor:711       Change metadata_group for clusters 24 to mua.\u001b[0m\n",
      "\u001b[0m16:26:30.500 [I] supervisor:711       Change metadata_group for clusters 25 to mua.\u001b[0m\n",
      "\u001b[0m16:26:33.355 [I] supervisor:711       Change metadata_group for clusters 26 to mua.\u001b[0m\n",
      "\u001b[0m16:26:36.572 [I] supervisor:711       Change metadata_group for clusters 27 to good.\u001b[0m\n",
      "\u001b[0m16:26:39.901 [I] supervisor:711       Change metadata_group for clusters 28 to mua.\u001b[0m\n",
      "\u001b[0m16:26:44.556 [I] supervisor:711       Change metadata_group for clusters 29 to good.\u001b[0m\n",
      "\u001b[0m16:26:49.027 [I] supervisor:711       Change metadata_group for clusters 30 to mua.\u001b[0m\n",
      "\u001b[0m16:26:54.091 [I] supervisor:711       Change metadata_group for clusters 31 to noise.\u001b[0m\n",
      "\u001b[0m16:27:01.772 [I] supervisor:711       Change metadata_group for clusters 32 to good.\u001b[0m\n",
      "\u001b[0m16:27:04.508 [I] supervisor:711       Change metadata_group for clusters 33 to good.\u001b[0m\n",
      "\u001b[0m16:27:07.187 [I] supervisor:711       Change metadata_group for clusters 34 to mua.\u001b[0m\n",
      "\u001b[0m16:27:11.060 [I] supervisor:711       Change metadata_group for clusters 35 to mua.\u001b[0m\n",
      "\u001b[0m16:27:14.700 [I] supervisor:711       Change metadata_group for clusters 36 to good.\u001b[0m\n",
      "\u001b[0m16:27:19.267 [I] supervisor:711       Change metadata_group for clusters 37 to good.\u001b[0m\n",
      "\u001b[0m16:27:22.180 [I] supervisor:711       Change metadata_group for clusters 38 to good.\u001b[0m\n",
      "\u001b[0m16:27:26.923 [I] supervisor:711       Change metadata_group for clusters 39 to mua.\u001b[0m\n",
      "\u001b[0m16:27:36.020 [I] supervisor:711       Change metadata_group for clusters 40 to good.\u001b[0m\n",
      "\u001b[0m16:27:39.091 [I] supervisor:711       Change metadata_group for clusters 41 to mua.\u001b[0m\n",
      "\u001b[0m16:27:42.140 [I] supervisor:711       Change metadata_group for clusters 42 to good.\u001b[0m\n",
      "\u001b[0m16:27:45.436 [I] supervisor:711       Change metadata_group for clusters 43 to good.\u001b[0m\n",
      "\u001b[0m16:27:51.596 [I] supervisor:711       Change metadata_group for clusters 44 to mua.\u001b[0m\n",
      "\u001b[0m16:27:55.900 [I] supervisor:711       Change metadata_group for clusters 45 to good.\u001b[0m\n",
      "\u001b[0m16:27:59.188 [I] supervisor:711       Change metadata_group for clusters 46 to mua.\u001b[0m\n",
      "\u001b[0m16:28:02.757 [I] supervisor:711       Change metadata_group for clusters 47 to good.\u001b[0m\n",
      "\u001b[0m16:28:06.100 [I] supervisor:711       Change metadata_group for clusters 48 to good.\u001b[0m\n",
      "\u001b[0m16:28:08.883 [I] supervisor:711       Change metadata_group for clusters 49 to mua.\u001b[0m\n",
      "\u001b[0m16:28:11.676 [I] supervisor:711       Change metadata_group for clusters 50 to good.\u001b[0m\n",
      "\u001b[0m16:28:15.316 [I] supervisor:711       Change metadata_group for clusters 51 to good.\u001b[0m\n",
      "\u001b[0m16:28:18.388 [I] supervisor:711       Change metadata_group for clusters 52 to mua.\u001b[0m\n",
      "\u001b[0m16:28:23.259 [I] supervisor:711       Change metadata_group for clusters 53 to mua.\u001b[0m\n",
      "\u001b[0m16:28:27.491 [I] supervisor:711       Change metadata_group for clusters 54 to mua.\u001b[0m\n",
      "\u001b[0m16:28:30.772 [I] supervisor:711       Change metadata_group for clusters 55 to good.\u001b[0m\n",
      "\u001b[0m16:28:33.852 [I] supervisor:711       Change metadata_group for clusters 56 to good.\u001b[0m\n",
      "\u001b[0m16:28:36.907 [I] supervisor:711       Change metadata_group for clusters 57 to mua.\u001b[0m\n",
      "\u001b[0m16:28:39.564 [I] supervisor:711       Change metadata_group for clusters 58 to good.\u001b[0m\n",
      "\u001b[0m16:28:42.852 [I] supervisor:711       Change metadata_group for clusters 59 to good.\u001b[0m\n",
      "\u001b[0m16:28:46.332 [I] supervisor:711       Change metadata_group for clusters 60 to good.\u001b[0m\n",
      "\u001b[0m16:28:51.924 [I] supervisor:711       Change metadata_group for clusters 61 to good.\u001b[0m\n",
      "\u001b[0m16:28:54.859 [I] supervisor:711       Change metadata_group for clusters 62 to good.\u001b[0m\n",
      "\u001b[0m16:28:58.772 [I] supervisor:711       Change metadata_group for clusters 63 to good.\u001b[0m\n",
      "\u001b[0m16:29:03.268 [I] supervisor:711       Change metadata_group for clusters 64 to good.\u001b[0m\n",
      "\u001b[0m16:29:07.156 [I] supervisor:711       Change metadata_group for clusters 65 to mua.\u001b[0m\n",
      "\u001b[0m16:29:09.899 [I] supervisor:711       Change metadata_group for clusters 66 to good.\u001b[0m\n",
      "\u001b[0m16:29:13.883 [I] supervisor:711       Change metadata_group for clusters 67 to good.\u001b[0m\n",
      "\u001b[0m16:29:16.747 [I] supervisor:711       Change metadata_group for clusters 68 to good.\u001b[0m\n",
      "\u001b[0m16:29:19.627 [I] supervisor:711       Change metadata_group for clusters 69 to good.\u001b[0m\n",
      "\u001b[0m16:29:22.499 [I] supervisor:711       Change metadata_group for clusters 70 to mua.\u001b[0m\n",
      "\u001b[0m16:29:25.979 [I] supervisor:711       Change metadata_group for clusters 71 to noise.\u001b[0m\n",
      "\u001b[0m16:29:28.747 [I] supervisor:711       Change metadata_group for clusters 72 to good.\u001b[0m\n",
      "\u001b[0m16:29:31.876 [I] supervisor:711       Change metadata_group for clusters 73 to mua.\u001b[0m\n",
      "\u001b[0m16:29:36.731 [I] supervisor:711       Change metadata_group for clusters 74 to good.\u001b[0m\n",
      "\u001b[0m16:29:39.467 [I] supervisor:711       Change metadata_group for clusters 75 to good.\u001b[0m\n",
      "\u001b[0m16:29:42.307 [I] supervisor:711       Change metadata_group for clusters 76 to mua.\u001b[0m\n",
      "\u001b[0m16:29:45.084 [I] supervisor:711       Change metadata_group for clusters 77 to good.\u001b[0m\n",
      "\u001b[0m16:29:47.988 [I] supervisor:711       Change metadata_group for clusters 78 to good.\u001b[0m\n",
      "\u001b[0m16:29:50.899 [I] supervisor:711       Change metadata_group for clusters 79 to mua.\u001b[0m\n",
      "\u001b[0m16:29:53.715 [I] supervisor:711       Change metadata_group for clusters 80 to good.\u001b[0m\n",
      "\u001b[0m16:29:56.476 [I] supervisor:711       Change metadata_group for clusters 81 to mua.\u001b[0m\n",
      "\u001b[0m16:29:59.332 [I] supervisor:711       Change metadata_group for clusters 82 to good.\u001b[0m\n",
      "\u001b[0m16:30:02.235 [I] supervisor:711       Change metadata_group for clusters 83 to good.\u001b[0m\n",
      "\u001b[0m16:30:05.412 [I] supervisor:711       Change metadata_group for clusters 84 to good.\u001b[0m\n",
      "\u001b[0m16:30:08.373 [I] supervisor:711       Change metadata_group for clusters 85 to mua.\u001b[0m\n",
      "\u001b[0m16:30:11.220 [I] supervisor:711       Change metadata_group for clusters 86 to good.\u001b[0m\n",
      "\u001b[0m16:30:14.028 [I] supervisor:711       Change metadata_group for clusters 87 to mua.\u001b[0m\n",
      "\u001b[0m16:30:17.003 [I] supervisor:711       Change metadata_group for clusters 88 to mua.\u001b[0m\n",
      "\u001b[0m16:30:19.811 [I] supervisor:711       Change metadata_group for clusters 89 to good.\u001b[0m\n",
      "\u001b[0m16:30:22.923 [I] supervisor:711       Change metadata_group for clusters 90 to mua.\u001b[0m\n",
      "\u001b[0m16:30:27.988 [I] supervisor:711       Change metadata_group for clusters 91 to good.\u001b[0m\n",
      "\u001b[0m16:30:30.987 [I] supervisor:711       Change metadata_group for clusters 92 to mua.\u001b[0m\n",
      "\u001b[0m16:30:33.980 [I] supervisor:711       Change metadata_group for clusters 93 to good.\u001b[0m\n",
      "\u001b[0m16:30:37.372 [I] supervisor:711       Change metadata_group for clusters 94 to mua.\u001b[0m\n",
      "\u001b[0m16:30:40.284 [I] supervisor:711       Change metadata_group for clusters 95 to good.\u001b[0m\n",
      "\u001b[0m16:30:43.723 [I] supervisor:711       Change metadata_group for clusters 96 to noise.\u001b[0m\n",
      "\u001b[0m16:30:46.884 [I] supervisor:711       Change metadata_group for clusters 97 to mua.\u001b[0m\n",
      "\u001b[0m16:30:49.875 [I] supervisor:711       Change metadata_group for clusters 98 to good.\u001b[0m\n",
      "\u001b[0m16:30:52.988 [I] supervisor:711       Change metadata_group for clusters 99 to mua.\u001b[0m\n",
      "\u001b[0m16:30:56.283 [I] supervisor:711       Change metadata_group for clusters 100 to good.\u001b[0m\n",
      "\u001b[0m16:30:59.563 [I] supervisor:711       Change metadata_group for clusters 101 to good.\u001b[0m\n",
      "\u001b[0m16:31:02.467 [I] supervisor:711       Change metadata_group for clusters 102 to mua.\u001b[0m\n",
      "\u001b[0m16:31:05.772 [I] supervisor:711       Change metadata_group for clusters 103 to mua.\u001b[0m\n",
      "\u001b[0m16:31:12.044 [I] supervisor:711       Change metadata_group for clusters 104 to good.\u001b[0m\n",
      "\u001b[0m16:31:15.148 [I] supervisor:711       Change metadata_group for clusters 105 to mua.\u001b[0m\n",
      "\u001b[0m16:31:18.291 [I] supervisor:711       Change metadata_group for clusters 106 to good.\u001b[0m\n",
      "\u001b[0m16:31:21.307 [I] supervisor:711       Change metadata_group for clusters 107 to good.\u001b[0m\n",
      "\u001b[0m16:31:24.429 [I] supervisor:711       Change metadata_group for clusters 108 to mua.\u001b[0m\n",
      "\u001b[0m16:31:27.556 [I] supervisor:711       Change metadata_group for clusters 109 to good.\u001b[0m\n",
      "\u001b[0m16:31:30.467 [I] supervisor:711       Change metadata_group for clusters 110 to mua.\u001b[0m\n",
      "\u001b[0m16:31:33.467 [I] supervisor:711       Change metadata_group for clusters 111 to good.\u001b[0m\n",
      "\u001b[0m16:31:36.252 [I] supervisor:711       Change metadata_group for clusters 112 to mua.\u001b[0m\n",
      "\u001b[0m16:31:38.979 [I] supervisor:711       Change metadata_group for clusters 113 to good.\u001b[0m\n",
      "\u001b[0m16:31:41.715 [I] supervisor:711       Change metadata_group for clusters 114 to mua.\u001b[0m\n",
      "\u001b[0m16:31:44.547 [I] supervisor:711       Change metadata_group for clusters 115 to good.\u001b[0m\n",
      "\u001b[0m16:31:50.315 [I] supervisor:711       Change metadata_group for clusters 116 to mua.\u001b[0m\n",
      "\u001b[0m16:31:53.372 [I] supervisor:711       Change metadata_group for clusters 117 to noise.\u001b[0m\n",
      "\u001b[0m16:31:56.389 [I] supervisor:711       Change metadata_group for clusters 118 to noise.\u001b[0m\n",
      "\u001b[0m16:31:59.067 [I] supervisor:711       Change metadata_group for clusters 119 to noise.\u001b[0m\n",
      "\u001b[0m16:32:01.940 [I] supervisor:711       Change metadata_group for clusters 120 to noise.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!phy template-gui E:/Florian/Data/Opto_2/M5686_3_g0/sorted/phy/params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2dd5666-eb5a-424b-a98d-7d7ad5ea9c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator IncrementalPCA from version 1.5.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'spikeinterface.full' has no attribute 'sorting_analyzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m si\u001b[38;5;241m.\u001b[39mset_global_job_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mglobal_job_kwargs)\n\u001b[0;32m     11\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m si\u001b[38;5;241m.\u001b[39mload_sorting_analyzer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/Florian/Data/batch3/M7_1/analyzer_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m tm \u001b[38;5;241m=\u001b[39m \u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msorting_analyzer\u001b[49m\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemplate_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spikeinterface.full' has no attribute 'sorting_analyzer'"
     ]
    }
   ],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import probeinterface as pi\n",
    "from pathlib import Path\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "global_job_kwargs = dict(n_jobs=4, chunk_duration=\"1s\",progress_bar=True)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "analyzer = si.load_sorting_analyzer(\"E:/Florian/Data/batch3/M7_1/analyzer_clean\")\n",
    "sorting(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a543dd19-5bb1-4876-a0de-d47637405dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5       0.00055\n",
      "6      0.000707\n",
      "9      0.000573\n",
      "10     0.000637\n",
      "11     0.000583\n",
      "         ...   \n",
      "261    0.000553\n",
      "262     0.00071\n",
      "263    0.000533\n",
      "264    0.000547\n",
      "265    0.000643\n",
      "Name: peak_to_valley, Length: 172, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tm = analyzer.compute('template_metrics')\n",
    "dat = tm.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ce67691-e394-43ae-b690-92305f0f66da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Florian\\Data\\batch2\\SNA-129853_2024-03-18_17-26-58\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129853_2024-03-18_17-26-58: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129853_2024-03-18_17-26-58\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129853_2024-03-19_13-36-52\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129853_2024-03-19_13-36-52: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129853_2024-03-19_13-36-52\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129853_2024-03-20_13-47-43\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129853_2024-03-20_13-47-43: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129853_2024-03-20_13-47-43\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129854_2024-03-12_17-24-42\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129854_2024-03-12_17-24-42: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129854_2024-03-12_17-24-42\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129854_2024-03-14_13-00-21\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129854_2024-03-14_13-00-21: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129854_2024-03-14_13-00-21\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129854_2024-03-14_17-43-27\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129854_2024-03-14_17-43-27: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129854_2024-03-14_17-43-27\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129855_2024-03-19_17-21-24\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129855_2024-03-19_17-21-24: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129855_2024-03-19_17-21-24\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129855_2024-03-20_18-34-43\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129855_2024-03-20_18-34-43: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129855_2024-03-20_18-34-43\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "E:\\Florian\\Data\\batch2\\SNA-129855_2024-03-21_15-40-17\\sorted\\waveforms_clean2\n",
      "Error processing E:\\Florian\\Data\\batch2\\SNA-129855_2024-03-21_15-40-17: [Errno 2] No such file or directory: 'E:\\\\Florian\\\\batch2\\\\SNA-129855_2024-03-21_15-40-17\\\\sorted\\\\sorter_output\\\\spike_times.npy'\n",
      "Empty DataFrame\n",
      "Columns: [Original_Index, Peak_to_Valley (ms)]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\spikeinterface\\core\\base.py:1117: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.100.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spikeinterface.full as si  # Assuming `si` is the module for loading the analyzer\n",
    "\n",
    "import pandas as pd  # Ensure Pandas is imported\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path  # For modern path handling\n",
    "\n",
    "def find_key_in_dict(d, key_to_find):\n",
    "    for key, value in d.items():\n",
    "        if key == key_to_find:\n",
    "            # If the value is a Pandas Series, convert it to a list\n",
    "            if isinstance(value, pd.Series):\n",
    "                return value.tolist()\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            result = find_key_in_dict(value, key_to_find)\n",
    "            if result:\n",
    "                return result\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    result = find_key_in_dict(item, key_to_find)\n",
    "                    if result:\n",
    "                        return result\n",
    "    return None\n",
    "\n",
    "# Base directory where subfolders are located\n",
    "base_dir = Path(\"E:/Florian/Data/batch3\")\n",
    "\n",
    "\n",
    "# Initialize a list to store all peak-to-valley values across subfolders\n",
    "all_peak_to_valley_ms = []\n",
    "original_indices = []  # To store the original indices for each value\n",
    "\n",
    "# Iterate through all subfolders\n",
    "for subfolder in base_dir.iterdir():  # Using pathlib to iterate\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            # Construct the full path and normalize it\n",
    "            analyzer_path = subfolder / \"analyzer\"\n",
    "            analyzer_path = os.path.normpath(analyzer_path)  # Normalize the path\n",
    "            print(analyzer_path)\n",
    "            # Load the analyzer for the current subfolder\n",
    "            analyzer = si.load_sorting_analyzer(str(analyzer_path))  # Convert to string if needed\n",
    "            tm = analyzer.compute('template_metrics')\n",
    "            dat = tm.data\n",
    "\n",
    "            # Extract peak-to-valley data\n",
    "            peak_to_valley_data = find_key_in_dict(dat['metrics'], 'peak_to_valley')\n",
    "            \n",
    "            # Check if peak_to_valley_data is found\n",
    "            if peak_to_valley_data:\n",
    "                # If it's a Pandas Series, convert it to a list\n",
    "                if isinstance(peak_to_valley_data, pd.Series):\n",
    "                    peak_to_valley_data = peak_to_valley_data.tolist()\n",
    "                \n",
    "                # Convert to milliseconds and append to the cumulative list\n",
    "                peak_to_valley_ms = [value * 1000 for value in peak_to_valley_data]\n",
    "                \n",
    "                # Append the values and their original indices\n",
    "                all_peak_to_valley_ms.extend(peak_to_valley_ms)\n",
    "                original_indices.extend([f\"{subfolder.name}_row_{i}\" for i in range(len(peak_to_valley_ms))])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {subfolder}: {e}\")\n",
    "\n",
    "# Filter values below 0.425ms (450ms) from the list\n",
    "filtered_peak_to_valley_ms = [\n",
    "    (index, value) for index, value in zip(original_indices, all_peak_to_valley_ms) if value < 0.425\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the filtered data, including the original indices\n",
    "df_filtered = pd.DataFrame(filtered_peak_to_valley_ms, columns=[\"Original_Index\", \"Peak_to_Valley (ms)\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_filtered)\n",
    "\n",
    "df_filtered.to_csv(\"C:/Users/deepl/Desktop/Batch3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d170b30d-7a63-4855-8f55-bc5be4fd3e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f79d03fddde4b0b9e50e38c9e4878bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sorting_KS4 = si.read_kilosort(\"E:/Florian/Data/batch2/SNA-129853_2024-03-18_17-26-58/sorted/sorter_output\")\n",
    "rec1 = si.read_openephys(\"E:/Florian/Data/batch2/SNA-129853_2024-03-18_17-26-58/Record Node 178\")\n",
    "import probeinterface as pi\n",
    "\n",
    "# download probe\n",
    "probe = pi.get_probe(manufacturer='cambridgeneurotech', probe_name='ASSY-77-H3')\n",
    "# add wiring\n",
    "probe.wiring_to_device('ASSY-77>Adpt.A64-Om32_2x-sm-NN>RHD2164')\n",
    "\n",
    "\n",
    "# set probe in place, ie, modify the current recording\n",
    "rec1.set_probe(probe, group_mode=\"by_shank\", in_place=True)\n",
    "                   \n",
    "analyzer = si.create_sorting_analyzer(Sorting_KS4, rec1, sparse=True, format=\"memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64156ff9-854a-4791-a17e-ceff4e543bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f277c25ea84a45a68c6e961eb43db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    }
   ],
   "source": [
    "analyzer.compute( [ 'random_spikes','waveforms','templates'],**global_job_kwargs)\n",
    "tm = analyzer.compute('template_metrics')\n",
    "dat = tm.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85c72d2e-68cf-4b0a-8d13-f65ac4be9a09",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 4 (466281426.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[94], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    peak_to_valley_data = peak_to_valley_data.tolist()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 4\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "18562c72-c526-4202-bc21-d165ffb6b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127307_2023-12-06_15-56-09\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127307_2023-12-06_15-56-09\\Record Node 134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216c8bccacab46d6a6b294441b53d85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6873e0b7245f499e9d7596d7e06a711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127307_2023-12-07_15-42-45\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127307_2023-12-07_15-42-45\\Record Node 134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70df9f29b154810b29e74a6eeaecb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/2281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb8742ce1c04878902c9917a9516477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/2281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127307_2023-12-08_14-26-27\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127307_2023-12-08_14-26-27\\Record Node 163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236bc4850da343ada10e997e54105b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/2201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e16806f2d5a496e93cb1e9fe07910c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/2201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-12_17-25-48\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-12_17-25-48\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e119e52f5ead4d1499ecf173c80e2ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54ed876330c4e7297cadae3e93e6589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-13_13-48-31\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-13_13-48-31\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41d07fc9a6b4d5991dd50c8b86ad3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a4c4fc8fba41dfb773514292f115dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-14_14-58-02\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-14_14-58-02\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae805ca02d584d66a39599dd94421140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed393608438453ebab304136e17c691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-15_13-29-22\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127315_2023-12-15_13-29-22\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea3d5313d9b4aa28fc9e6acc44323fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042d9d149f254b78880082fc7642b6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-13_16-26-56\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-13_16-26-56\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6819a3f16ecc4fc7b8e60e48073df814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245feabfd43a411e92b5714cb6a590a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-14_17-14-06\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-14_17-14-06\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ed4317428c4870aa3051d89e02360e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c099525912a452e91991c91e5b6f0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\spikeinterface\\postprocessing\\template_metrics.py:388: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ptratio = template_single[peak_idx] / template_single[trough_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-15_15-59-02\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-15_15-59-02\\Record Node 171\n",
      "Error processing E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-15_15-59-02: This dataset is multi-block. Spikeinterface can load one block at a time. Use 'block_index' to select the block to be loaded.\n",
      "Sorting Output Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-16_14-41-01\\sorted\\sorter_output\n",
      "Record Node Path: E:\\Florian\\Data\\batch1\\SNA-127316_2023-12-16_14-41-01\\Record Node 171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319b41f800ce40c6a95f5ddea174bb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity:   0%|          | 0/1896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ec71dc17974ad6920df13181b0e7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms:   0%|          | 0/1896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Original_Index  Peak_to_Valley (ms)\n",
      "0     SNA-127307_2023-12-06_15-56-09_row_0             0.000000\n",
      "1     SNA-127307_2023-12-06_15-56-09_row_2             0.000000\n",
      "2     SNA-127307_2023-12-06_15-56-09_row_3             0.000000\n",
      "3     SNA-127307_2023-12-06_15-56-09_row_4             0.000000\n",
      "4     SNA-127307_2023-12-06_15-56-09_row_5             0.000000\n",
      "..                                     ...                  ...\n",
      "398  SNA-127316_2023-12-16_14-41-01_row_20             0.353333\n",
      "399  SNA-127316_2023-12-16_14-41-01_row_23             0.040000\n",
      "400  SNA-127316_2023-12-16_14-41-01_row_30             0.043333\n",
      "401  SNA-127316_2023-12-16_14-41-01_row_49             0.350000\n",
      "402  SNA-127316_2023-12-16_14-41-01_row_50             0.000000\n",
      "\n",
      "[403 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:196: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\deepl\\anaconda3\\envs\\si_env\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:199: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spikeinterface.full as si  # Assuming `si` is the module for loading the analyzer\n",
    "\n",
    "import pandas as pd  # Ensure Pandas is imported\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path  # For modern path handling\n",
    "\n",
    "def find_key_in_dict(d, key_to_find):\n",
    "    for key, value in d.items():\n",
    "        if key == key_to_find:\n",
    "            # If the value is a Pandas Series, convert it to a list\n",
    "            if isinstance(value, pd.Series):\n",
    "                return value.tolist()\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            result = find_key_in_dict(value, key_to_find)\n",
    "            if result:\n",
    "                return result\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    result = find_key_in_dict(item, key_to_find)\n",
    "                    if result:\n",
    "                        return result\n",
    "    return None\n",
    "base_dir = Path(\"E:/Florian/Data/batch1\")\n",
    "all_peak_to_valley_ms = []\n",
    "original_indices = []  \n",
    "\n",
    "for subfolder in base_dir.iterdir():  # Using pathlib to iterate\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            # Construct the path for sorting output (fixed)\n",
    "            sorting_output_path = subfolder / \"sorted/sorter_output\"\n",
    "            sorting_output_path = os.path.normpath(sorting_output_path)  # Normalize the path\n",
    "\n",
    "            # Search for a subfolder containing 'Record Node' and a number\n",
    "            record_node_folders = [f for f in subfolder.iterdir() if \"Record Node\" in f.name]\n",
    "            if not record_node_folders:\n",
    "                print(f\"No 'Record Node' found in {subfolder}. Skipping...\")\n",
    "                continue\n",
    "            record_node_folder = record_node_folders[0]\n",
    "            record_node_path = os.path.normpath(record_node_folder)  # Normalize the path\n",
    "            \n",
    "            # Print paths for debugging\n",
    "            print(f\"Sorting Output Path: {sorting_output_path}\")\n",
    "            print(f\"Record Node Path: {record_node_path}\")\n",
    "\n",
    "            # Load Sorting (Kilosort) data\n",
    "            Sorting_KS4 = si.read_kilosort(str(sorting_output_path))  # Ensure path is passed as string\n",
    "\n",
    "            # Load OpenEphys data\n",
    "            rec1 = si.read_openephys(str(record_node_path))\n",
    "\n",
    "            # Download probe\n",
    "            probe = pi.get_probe(manufacturer='cambridgeneurotech', probe_name='ASSY-77-H3')\n",
    "            \n",
    "            # Add wiring to device\n",
    "            probe.wiring_to_device('ASSY-77>Adpt.A64-Om32_2x-sm-NN>RHD2164')\n",
    "\n",
    "            # Set the probe for the current recording\n",
    "            rec1.set_probe(probe, group_mode=\"by_shank\", in_place=True)\n",
    "\n",
    "            # Create the sorting analyzer with the loaded data\n",
    "            analyzer = si.create_sorting_analyzer(Sorting_KS4, rec1, sparse=True, format=\"memory\")\n",
    "\n",
    "            # Compute the necessary metrics\n",
    "            global_job_kwargs = {}  # You can define the global arguments here if needed\n",
    "            analyzer.compute(['random_spikes', 'waveforms', 'templates'], **global_job_kwargs)\n",
    "\n",
    "            # Compute 'template_metrics'\n",
    "            tm = analyzer.compute('template_metrics')\n",
    "            dat = tm.data\n",
    "\n",
    "            peak_to_valley_data = find_key_in_dict(dat['metrics'], 'peak_to_valley')\n",
    "            \n",
    "            # Check if peak_to_valley_data is found\n",
    "            if peak_to_valley_data:\n",
    "                # If it's a Pandas Series, convert it to a list\n",
    "                if isinstance(peak_to_valley_data, pd.Series):\n",
    "                    peak_to_valley_data = peak_to_valley_data.tolist()\n",
    "                \n",
    "                # Convert to milliseconds and append to the cumulative list\n",
    "                peak_to_valley_ms = [value * 1000 for value in peak_to_valley_data]\n",
    "                \n",
    "                # Append the values and their original indices\n",
    "                all_peak_to_valley_ms.extend(peak_to_valley_ms)\n",
    "                original_indices.extend([f\"{subfolder.name}_row_{i}\" for i in range(len(peak_to_valley_ms))])\n",
    "        \n",
    "        except Exception as e:\n",
    "                print(f\"Error processing {subfolder}: {e}\")\n",
    "\n",
    "# Filter values below 0.425ms (450ms) from the list\n",
    "filtered_peak_to_valley_ms = [\n",
    "    (index, value) for index, value in zip(original_indices, all_peak_to_valley_ms) if value < 0.425\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the filtered data, including the original indices\n",
    "df_filtered = pd.DataFrame(filtered_peak_to_valley_ms, columns=[\"Original_Index\", \"Peak_to_Valley (ms)\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_filtered)\n",
    "\n",
    "df_filtered.to_csv(\"C:/Users/deepl/Desktop/Batch1.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
